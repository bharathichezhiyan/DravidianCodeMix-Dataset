{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "M5EkrzM-1_JU"
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "q5m6vPje2hiy",
    "outputId": "09f7d3cc-88a7-4a7b-e8ba-775406b76ac5"
   },
   "outputs": [],
   "source": [
    "#train_directory = \"https://raw.githubusercontent.com/ahmedhammad97/offensive-dataset/master/offenseval-training-v1.tsv?token=AVUYG3_VUnrSFPn3HUsRuK-seXmB92NIks5cPnJ7wA%3D%3D\"\n",
    "#print(\"Reading Dataset...\")\n",
    "train_data = pd.read_csv('offensive.csv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ucASsaD24HvU"
   },
   "outputs": [],
   "source": [
    "tweets = train_data[[\"tweet\"]]\n",
    "subtask_a_labels = train_data[[\"subtask_a\"]]\n",
    "subtask_b_labels = train_data.query(\"subtask_a == 'OFF'\")[[\"subtask_b\"]]\n",
    "subtask_c_labels = train_data.query(\"subtask_b == 'TIN'\")[[\"subtask_c\"]]\n",
    "\n",
    "clean_tweets = copy.deepcopy(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13210</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13211</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13212</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13213</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13214</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13215</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13216</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13217</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13218</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13219</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13220</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13221</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13222</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13223</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13224</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13225</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13226</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13227</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13228</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13229</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13230</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13231</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13232</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13233</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13234</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subtask_a\n",
       "0           OFF\n",
       "1           OFF\n",
       "2           NOT\n",
       "3           OFF\n",
       "4           NOT\n",
       "5           OFF\n",
       "6           OFF\n",
       "7           OFF\n",
       "8           NOT\n",
       "9           OFF\n",
       "10          NOT\n",
       "11          NOT\n",
       "12          OFF\n",
       "13          NOT\n",
       "14          NOT\n",
       "15          NOT\n",
       "16          NOT\n",
       "17          NOT\n",
       "18          NOT\n",
       "19          OFF\n",
       "20          OFF\n",
       "21          NOT\n",
       "22          OFF\n",
       "23          OFF\n",
       "24          NOT\n",
       "25          OFF\n",
       "26          NOT\n",
       "27          NOT\n",
       "28          NOT\n",
       "29          OFF\n",
       "...         ...\n",
       "13210       NOT\n",
       "13211       NOT\n",
       "13212       OFF\n",
       "13213       NOT\n",
       "13214       NOT\n",
       "13215       NOT\n",
       "13216       NOT\n",
       "13217       OFF\n",
       "13218       NOT\n",
       "13219       NOT\n",
       "13220       NOT\n",
       "13221       NOT\n",
       "13222       NOT\n",
       "13223       OFF\n",
       "13224       NOT\n",
       "13225       NOT\n",
       "13226       NOT\n",
       "13227       OFF\n",
       "13228       NOT\n",
       "13229       NOT\n",
       "13230       NOT\n",
       "13231       NOT\n",
       "13232       NOT\n",
       "13233       NOT\n",
       "13234       NOT\n",
       "13235       OFF\n",
       "13236       NOT\n",
       "13237       OFF\n",
       "13238       OFF\n",
       "13239       NOT\n",
       "\n",
       "[13240 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean_tweets\n",
    "subtask_a_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Tamil_first_ready_for_sentiment.csv',sep='\\t',names=['category','text'])\n",
    "text=df[['text']]\n",
    "labels=df[['category']]\n",
    "#text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3FKc2Kp4MxS"
   },
   "outputs": [],
   "source": [
    "##PREPROCESSING##\n",
    "#tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "EJHU2cRi5SH1",
    "outputId": "7208c3c9-7fc1-4862-e923-89db2eeea598"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to stopwords...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt', 'stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r7qm0EYh5bhB"
   },
   "outputs": [],
   "source": [
    "def take_data_to_shower(tweet):\n",
    "    noises = ['URL', '@USER', '\\'ve', 'n\\'t', '\\'s', '\\'m']\n",
    "\n",
    "    for noise in noises:\n",
    "        tweet = tweet.replace(noise, '')\n",
    "\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', tweet)\n",
    "\n",
    "\n",
    "def tokenize(tweet):\n",
    "    lower_tweet = tweet.lower()\n",
    "    return word_tokenize(lower_tweet)\n",
    "\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    clean_tokens = []\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    for token in tokens:\n",
    "        if token not in stopWords:\n",
    "            if token.replace(' ', '') != '':\n",
    "                if len(token) > 1:\n",
    "                    clean_tokens.append(token)\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def stem_and_lem(tokens):\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        token = wordnet_lemmatizer.lemmatize(token)\n",
    "        token = lancaster_stemmer.stem(token)\n",
    "        if len(token) > 1:\n",
    "            clean_tokens.append(token)\n",
    "    return clean_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "jSwKKcS35gmW",
    "outputId": "d7b0f0e6-2bf1-48fe-d95d-6ab2e74d5573"
   },
   "outputs": [],
   "source": [
    "# tqdm.pandas(desc=\"Cleaning Data Phase I...\")\n",
    "# clean_tweets['tweet'] = tweets['tweet'].progress_apply(take_data_to_shower)\n",
    "\n",
    "# tqdm.pandas(desc=\"Tokenizing Data...\")\n",
    "# clean_tweets['tokens'] = clean_tweets['tweet'].progress_apply(tokenize)\n",
    "\n",
    "# tqdm.pandas(desc=\"Cleaning Data Phase II...\")\n",
    "# clean_tweets['tokens'] = clean_tweets['tokens'].progress_apply(remove_stop_words)\n",
    "\n",
    "# tqdm.pandas(desc=\"Stemming And Lemmatizing\")\n",
    "# clean_tweets['tokens'] = clean_tweets['tokens'].progress_apply(stem_and_lem)\n",
    "\n",
    "# text_vector = clean_tweets['tokens'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tqdm.pandas(desc=\"Cleaning Data Phase I...\")\n",
    "# clean_tweets['tweet'] = tweets['tweet'].progress_apply(take_data_to_shower)\n",
    "\n",
    "# tqdm.pandas(desc=\"Tokenizing Data...\")\n",
    "# clean_tweets['tokens'] = clean_tweets['tweet'].progress_apply(tokenize)\n",
    "\n",
    "# tqdm.pandas(desc=\"Cleaning Data Phase II...\")\n",
    "# clean_tweets['tokens'] = clean_tweets['tokens'].progress_apply(remove_stop_words)\n",
    "\n",
    "# tqdm.pandas(desc=\"Stemming And Lemmatizing\")\n",
    "# clean_tweets['tokens'] = clean_tweets['tokens'].progress_apply(stem_and_lem)\n",
    "\n",
    "# text_vector = clean_tweets['tokens'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Y2fN2MC8Ara"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning Data Phase I...: 100%|██████████| 15744/15744 [00:00<00:00, 76594.52it/s]\n",
      "Tokenizing Data...: 100%|██████████| 15744/15744 [00:02<00:00, 5267.02it/s]\n",
      "Cleaning Data Phase II...: 100%|██████████| 15744/15744 [00:03<00:00, 4664.21it/s]\n",
      "Stemming And Lemmatizing: 100%|██████████| 15744/15744 [00:02<00:00, 5780.73it/s]\n"
     ]
    }
   ],
   "source": [
    "##EMBEDDING##\n",
    "clean_texts = copy.deepcopy(text)\n",
    "tqdm.pandas(desc=\"Cleaning Data Phase I...\")\n",
    "clean_texts['text'] = text['text'].progress_apply(take_data_to_shower)\n",
    "\n",
    "tqdm.pandas(desc=\"Tokenizing Data...\")\n",
    "clean_texts['tokens'] = clean_texts['text'].progress_apply(tokenize)\n",
    "\n",
    "tqdm.pandas(desc=\"Cleaning Data Phase II...\")\n",
    "clean_texts['tokens'] = clean_texts['tokens'].progress_apply(remove_stop_words)\n",
    "\n",
    "tqdm.pandas(desc=\"Stemming And Lemmatizing\")\n",
    "clean_texts['tokens'] = clean_texts['tokens'].progress_apply(stem_and_lem)\n",
    "\n",
    "text_vector = clean_texts['tokens'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UoVtfyxb5lp3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfid(text_vector):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    untokenized_data =[' '.join(tweet) for tweet in tqdm(text_vector, \"Vectorizing...\")]\n",
    "    vectorizer = vectorizer.fit(untokenized_data)\n",
    "    vectors = vectorizer.transform(untokenized_data).toarray()\n",
    "    return vectors\n",
    "  \n",
    "def get_vectors(vectors, labels, keyword):\n",
    "    if len(vectors) != len(labels):\n",
    "        print(\"Unmatching sizes!\")\n",
    "        return\n",
    "    result = list()\n",
    "    for vector, label in zip(vectors, labels):\n",
    "        if label == keyword:\n",
    "            result.append(vector)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fuA8n1dt7VJe",
    "outputId": "6a9a0365-8a2e-4ac9-dc4f-28083de3f0ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing...: 100%|██████████| 15744/15744 [00:00<00:00, 667371.97it/s]\n"
     ]
    }
   ],
   "source": [
    "vectors_a = tfid(text_vector) # Numerical Vectors A\n",
    "labels_a = labels['category'].values.tolist() # Subtask A Labels\n",
    "\n",
    "# vectors_b = get_vectors(vectors_a, labels_a, \"OFF\") # Numerical Vectors B\n",
    "# labels_b = subtask_b_labels['subtask_b'].values.tolist() # Subtask A Labels\n",
    "\n",
    "# vectors_c = get_vectors(vectors_b, labels_b, \"TIN\") # Numerical Vectors C\n",
    "# labels_c = subtask_c_labels['subtask_c'].values.tolist() # Subtask A Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2xxecoX_7dWS"
   },
   "outputs": [],
   "source": [
    "##CLASSIFING##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vllyKsR475YC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def classify(vectors, labels, type=\"DT\"):\n",
    "    # Random Splitting With Ratio 3 : 1\n",
    "    train_vectors, test_vectors, train_labels, test_labels = train_test_split(vectors, labels, random_state=5, test_size=0.2)\n",
    "\n",
    "    # Initialize Model\n",
    "    classifier = None\n",
    "    if(type==\"MNB\"):\n",
    "        classifier = MultinomialNB(alpha=0.7)\n",
    "        classifier.fit(train_vectors, train_labels)\n",
    "    elif(type==\"KNN\"):\n",
    "        classifier = KNeighborsClassifier(n_jobs=4)\n",
    "        params = {'n_neighbors': [3,5,7,9], 'weights':['uniform', 'distance']}\n",
    "        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
    "        classifier.fit(train_vectors, train_labels)\n",
    "        classifier = classifier.best_estimator_\n",
    "    elif(type==\"SVM\"):\n",
    "        classifier = SVC()\n",
    "        classifier = GridSearchCV(classifier, {'C':[0.001, 0.01, 0.1, 1, 10]}, cv=3, n_jobs=4)\n",
    "        classifier.fit(train_vectors, train_labels)\n",
    "        classifier = classifier.best_estimator_\n",
    "    elif(type==\"DT\"):\n",
    "        classifier = DecisionTreeClassifier(max_depth=800, min_samples_split=5)\n",
    "        params = {'criterion':['gini','entropy']}\n",
    "        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
    "        classifier.fit(train_vectors, train_labels)\n",
    "        classifier = classifier.best_estimator_\n",
    "    elif(type==\"RF\"):\n",
    "        classifier = RandomForestClassifier(max_depth=800, min_samples_split=5)\n",
    "        params = {'n_estimators': [n for n in range(50,200,50)], 'criterion':['gini','entropy'], }\n",
    "        classifier = GridSearchCV(classifier, params, cv=3, n_jobs=4)\n",
    "        classifier.fit(train_vectors, train_labels)\n",
    "        classifier = classifier.best_estimator_\n",
    "    elif(type==\"LR\"):\n",
    "        classifier = LogisticRegression(multi_class='auto', solver='newton-cg',)\n",
    "        classifier = GridSearchCV(classifier, {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"]}, cv=3, n_jobs=4)\n",
    "        classifier.fit(train_vectors, train_labels)\n",
    "        classifier = classifier.best_estimator_\n",
    "    else:\n",
    "        print(\"Wrong Classifier Type!\")\n",
    "        return\n",
    "\n",
    "    accuracy = accuracy_score(train_labels, classifier.predict(train_vectors))\n",
    "    print(\"Training Accuracy:\", accuracy)\n",
    "    test_predictions = classifier.predict(test_vectors)\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "    print(\"Confusion Matrix:\", )\n",
    "    print(confusion_matrix(test_labels, test_predictions))\n",
    "    print(classification_report([i for i in test_labels], \n",
    "                            [i for i in test_predictions]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "nUc62OmA8KrF",
    "outputId": "af7faf87-d5ff-4c7d-987a-c35c0c3839c2"
   },
   "outputs": [],
   "source": [
    "# print(\"\\nBuilding Model Subtask A...\")\n",
    "# classify(vectors_a[1000:2000], labels_a[1000:2000], \"SVM\") # {MNB, KNN, SVM, DT, RF, LR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Model SVM...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuilding Model SVM...\")\n",
    "classify(vectors_a, labels_a, \"SVM\") # {MNB, KNN, SVM, DT, RF, LR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "id": "ASbewYhr8Yha",
    "outputId": "da461e12-13c6-4c7d-f090-9c37c5eb3bac"
   },
   "outputs": [],
   "source": [
    "print(\"\\nBuilding Model MNB...\")\n",
    "classify(vectors_a, labels_a, \"MNB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "TAF7Vz3nlWj8",
    "outputId": "3aa22dd2-07e6-4768-ec44-80d18b697687"
   },
   "outputs": [],
   "source": [
    "print(\"\\nBuilding Model KNN...\")\n",
    "classify(vectors_a, labels_a, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QfhWGl8zCFt3"
   },
   "outputs": [],
   "source": [
    "print(\"\\nBuilding Model DT...\")\n",
    "classify(vectors_a, labels_a, \"DT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nBuilding Model RF...\")\n",
    "classify(vectors_a, labels_a, \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"\\nBuilding Model LR...\")\n",
    "classify(vectors_a, labels_a, \"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Offensive_tweets.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
